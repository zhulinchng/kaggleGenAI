Q&A segment will be a continuation of all of the things that you've been seeing on Discord um and I want to make sure to thank our wonderful uh generative AI discourse moderators poong cause Mark Eric Irwin miles and many of the other folks that have been kind of diving in and helping um make sure to Emoji thank them um and also uh also text thank them I I know that they've been busy uh and working in every single time zone the the Sun never sleeps on uh on General of AIA Google um so with that uh day number one um our expert host panel includes Logan Kilpatrick um who's the product lead for AI studio um Lee Boonstra um Alexi severon um Majid Amin baroon um Daniel manowitz H Chuck and an not um and I'm excited to have everyone here today um uh as an example um uh we have folks that have worked on reinforcement learning with human feedback um people who have been working on uh people who have been working on the Gemini app model inference um making it performant and efficient um technical directors from the office of our CTO um and even more um and with that I'm going to go ahead uh and uh and start with our first question um to Logan good to see you Logan how are you doing I'm doing awesome I'm excited to both hear the questions from folks but also hear some of the questions for some of the other uh panelists excellent very cool and the ml developer team has been shipping so much just recently it feels like every day I log on to to LinkedIn or to Twitter um or X or or whatever it's called now it seems like there have been new product releases um tell me which features you're most excited about um and which uh which have already launched or about to launch yeah that's a great question I think the two that are top of mind right now um about two weeks ago we landed grounding with Google search which is really exciting so I think this will surely be a topic of conversation over the next five days but llms and hallucination and grounding and rag uh grounding with Google search is is a way to sort of tap into the the search index that Google has and bring some of that information to ground answers rather ground questions uh that that users of of LMS have so super super interesting there's yeah if you just Google search grounding with Google uh you'll you'll find some some of the links across the developer side the Enterprise side um so excited about that happy to talk more about it the other thing is Friday we landed uh open AI compatibility which I'm super excited so for developers who are already using the open Ai sdks and libraries um you can now try out the Gemini models using those sdks and libraries by only changing like three lines of code or something like that so um excited to to sort of reduce the friction for developers who are excited about using Gemini and yeah hopefully we'll we'll hear a bunch of great feedback about that launch as well excellent I personally have been really really jazzed for uh for the open AI API compatibility um it's been really really straightforward to convert notebooks that might have been preferring the open AI apis to just using Gemini um and to be able to compare and contrast between the two um so those are those are definitely two exciting things that have come out um and I encourage everyone to take a look at the uh at the open AI compatibility and then also the search grounding features um within AI Studio I think it's especially cool that you can use search grounding with even some of our smaller models uh like Gemini 1.5 Flash and Gemini 1.5 flab um I've been loving both of those for my projects especially things um requiring uh you know code execution or are super fast inference um uh tell me tell me a little bit about those two Flash series of models um and how you can do so much so quickly um with a very small cost footprint yeah so we we landed um after a bunch of work internally we landed The Flash AP model which is the smallest uh sort of Gemini hosted model that that exists um 8 billion parameter model and really pushes sort of the envelope of you know compute intelligence per dollar I think we've seen a bunch of those tweets page you and I have been ones putting those out but it's like two three cents per million tokens if you using cash tokens it's one cent per million tokens so just like an absolute an absolute crazy amount of intelligence that you can get for um for a couple of cents and I think the the sort of interesting thread of why those small models are interesting to me is because a lot of the you know Frontier use cases of AI are actually founded by uh by cost and token cost so like there's a lot of things like the developers actually inherently have a disincentive to build because it's like you you are paying the more AI the more AI you build into something the more you pay um and I think flash AP is like a great example of sort of pushing the frontier of trying to like remove that barrier so that developers can actually just build the things that they're excited about um and not need to sort of worry about you know the incremental amount of costs that they take on yeah amazing um and then one last question and then we're going to we're going to bring in some more folks to to discuss uh especially the production applications of of generative AI um you know deep Minds multimodal models uh I'm thinking particularly of imagine and vo for video generation Lia for music generation they unlock some really interesting new applications especially when coupled with the Gemini models um what has been your favorite application for for these kind of um multimodal output scenarios um and uh and um how do you think about coupling those with a gem apis yeah that's a really good question I I think the it is so hard to create um or rather I think text as sort of like an output mechanism for models I think like you know developers have found the interesting things to do with text like you know you build chat apps you sort of process data you do a bunch of stuff it does feel like we're actually still pretty early in this like what are the real actual production use cases of multimodal output creation um both in image video and others like I I feel like we just like haven't uh there hasn't been as much ubiquitous access to those models and thus like developers haven't had enough time to really build interesting things um I think the the thing that I'm personally excited about is is actually like comparing or or rather bringing uh text to life using those things like I think there's just like a lot of text in the world and humans in a lot of cases are inherently visual um and I think if it's possible and we sort of saw this with notebook LM actually as an example like if you can take a bunch of written content and bring it to life in audio form it actually makes the experience like 10x better in some contexts and I can imagine you know the sort of uh I'm not on the notbook gan team so this isn't actually a product vision for them but uh you know you bring to life sort of the the huge Corpus of text documents you have in the form of a video instead of just like an audio conversation I think that's like a that's a crazy future world to to live in and I'm I I think it is going to be models like imagin in vo that actually um enable that mission to come to fruition so it's going to be um it's gonna be exciting and I see your lights just turned off this is folks who don't know the Google offices the lights turn off all the time so it's a it's a normal occurrence yes and the uh and I I love that use case as well about how you can uh sort of bring life to text documents or even to to things like GitHub code bases through video through audio through images um I think my favorite use case so far has been doing book trailers um so so taking in taking in a PDF book automatically you know segmenting it into into different into different prompts um and then creating a short video to describe the book I I I love it um and and would love to see love would see uh love to see more of that thing um so thank you so much this is this has been wonderful um I am going to pull up uh pull up our next question um which is all about reinforcement learning with human feedback um and I'm going to bring uh bring on stage Alexi Alexi you're uh correct me if I'm wrong you're our lead for rhf for the for the Gemini app is that correct correct excellent thank you and uh I'm really uh I'm really curious to learn more about how uh the Gemini app is using rhf to help improve its responses um uh how do how does that process work how do you uh sort of use user feedback um to to improve improve the models over time or to improve your app experiences over time yeah um so I mean it's a very big top uh so maybe it makes sense uh to say a few words uh on a high level how the llms are fine-tuned so like the def Factor recipe today for post trining uh state-of-the-art llms is U to do it in two steps sft and rlf sft is supervised fine tuning where we use demonstration data and I mean typically to get good results uh this data needs to be of extremely high quality and it's most often produced by human experts so it's quite expensive to get rhf is a a powerful technique uh for aligning models to human preferences um yeah think of like making responses safer more helpful or factually correct and uh it's a bit more complicated than sft um but in brief uh we use a collection of input prompts to generate responses from the model that we're trying to fine-tune and then un use another model called reward model to penalize responses uh that are bad and reward the good ones and this reward model can be uh fine-tuned on different types of preference data so typically it's human preference data this is what a relish stands for like human uh reinforcement learning with human feedback um and this data can be collected from Human expert raters just like um sft data or it can also come from um uh user feedback so people like users interacting with Gemini uh chatbot on on the desktop or their mobiles so the thumbs up thumbs down uh interactions that we that we uh do with the models actually actually lead to better performance over time is that what you're saying yeah it helps us to produce models that are perceived as more helpful uh to our users so this is kind of one of the primary mechanisms to align uh our models uh to human preferences amazing that's that's really exciting to hear and I know folks have been learning all about rhf and all about improving models with things like human feedback um through the through the course of their readings um the next the next question is um uh uh from from one of our folks at Google so I've been wondering um since large language models learn from massive sources of data do large language models simply interpolate within their training data or can they go beyond to make new discoveries um and Aman how about how about you answer that I I think uh and then also uh welcome back I I believe I believe used to work at at Google deepmind and now you've started your own company is that correct uh uh no actually uh I I used to work at Deep mine but now I'm I'm I'm work for a new company called quader uh so yeah I I industry actually and is that in the finance industry and quantitative quantitative research yeah quantitive research these days you can basically apply AI in any application and finance has been also very very interesting application very very cool so so tell me uh tell me about the uh models making new discoveries have you have you that that is the case or are they just next token predictors yeah yeah I mean that that's a like a philosophical and interesting question uh but uh I believe the answer to this question is is yes the models can definitely go beyond uh their training data and uh the technique that will uh let them do that is uh wellknown uh search basically so uh the first example uh that we we saw that was the uh work uh called fund search uh that he did in deep uh uh end of like 23 and uh in font search we basically used an llm to search in the space of uh Python programming language uh for let's say a very hard problem like an NP problem NP hard problem and uh the way it worked was that they basically paired an a large language model with a efficient uh let's say evaluator that would provide feedback on how good uh each solution is and then with an iterative algorithm it managed to discover new solutions for some open problems in Computer Science and Mathematics uh so the core idea was that the llm would basically propose uh let's say solu Solutions like a piece of code for a for a particular problem and then the evaluator would give some scores on on how good that solution is and then there was an evolutionary algorithm that would uh select the best solutions that the llm discovered so far it would present it to the llm and it would basically ask it to make it better so in some ways uh it was doing Best Shot prompting uh like a automatic way of prompting the llm we would just show it good Solutions we ask them to make better the LM comes up with something new we would show it again and and ask it to make it better and and this sort of process is repeated about 100 thousands of times and then the code that it starts with is something like a return zero and it would like evolve into something very interesting and complicated that would actually uh result in some new uh let's say solutions for open problems and it because this uh basically Solutions has never discovered by humans before it kind of shows that the llm discovered something new that wasn't in it training data and uh this concept has been like uh basically studied a lot like in the history of AI and recent examples uh recently it has become very popular in the modern llms and uh it's referred to by for example test time compute or inference scaling and uh other terminologies and and the way it works is that like uh you can basically uh uh when you ask a question for the llm uh you allow the M to think and the LM would basically generate multiple scenarios and generates like chains of reasonings and then it has a mechanism to understand that okay which one is more likely and then it would just iteratively repeat this process and by simulating different possibilities it basically search at inference time and with by searching at inference it it can manage to like basically discover something new because it can put a strap its own knowledge very cool um and and that makes me that makes me think that we've only just begun to tap into the kinds of things that that large language models will be able to accomplish or to achieve ex really really excited to to see what the next couple of years hold exactly like I I think like Rich sat puts it the best way we have like in the bitter lesson that like we have two techniques that basically scale infinitely uh the first one is like learning which sort of be cracked by like you know large amount of data and and just like the like the first let's say Generations of models we saw and then the next technique is search and I think we are at the moment where where people are are starting to scale search and uh throwing compute at inference time and that's going to unlock a whole lot of new capabilities especially for like reasoning capabilities and that's going to be very exciting excellent um thank you so much I I think I I think this also leads uh is a great lead to our next question um which was from our Discord app um uh uh and I think this is for Majid uh Majid you're our inference lead for the Gemini app uh yep excellent and uh so so this question is is really um is really all about uh can larger language models be used to train smaller ones um can we take some of this knowledge that that might have been obtained from a larger model and use it to improve smaller models um uh things that uh kind of like the model that we embed within many of the product surfaces at Google um what are your thoughts about that uh yeah absolutely uh yeah absolutely we can and yeah it's a very good question um this uh technique is called distillation uh it's a very popular inference optimization technique um and the main idea is as you said paig you you would take the knowledge from a very large model that might be too expensive to serve but has very good quality and then you would distill that knowledge into a smaller model um that can can retain a good amount of that knowledge and quality of the big model but is a at a size that you can serve um so yeah like I said it's a technique called distillation uh lots of active research and practice in in distillation um there are multiple types that you can distill Knowledge from a larger model to a smaller model um one of them is called Data distillation where you use the larger model to generate a lot of synthetic training data for the smaller model and then you just train the smaller model on that um another way we can do this is more fine grain where we try to align the token distribution for the smaller llm to be closer to the larger model and that's what we would call Knowledge distillation um and we talk about those types in the white paper uh that we're publishing with this with this class as well and the third type that I would mention is called on policy distillation where kind of going back to your discussion with Alexa on rhf we would be able to use the same reinforcement learning framework but in this case we would uh use the teacher model which is the larger model uh to effectively score output from the smaller model and use those scores to train and align the smaller model to to the larger model um so yeah distillation has multiple types they're all used in practice based on the use case and based on what we need um and actually I think in tomorrow's class one of those cases will be presented as well I think uh one of the authors for the gecko paper which uses llms to uh improve to produce much better embedding models using llms so you would use the llm to essentially produce query uh pass query uh queries for for passages and then you would train the embedding models on that and you would get a much much better performance I think it was something like 7x better for those embedding models of the same size using distillation um and I think jinuk Lee who's the author on that paper will be present in the vectors and embedding classes tomorrow very cool I I think uh I think what you just discussed will also be very helpful for the folks uh in our first pop quiz which should be coming up toward Wards the end of the hour um so I hope everybody was paying attention um all about the the great ways that you can uh use distillation to improve smaller models um and mechanisms uh mechanisms to improve uh to improve models at large um thank you so much um yep and next next question um I believe this is for you and Aunt um what are some approaches to evaluating large models you know we we've got uh we've got these models that have been created some larger versions some smaller Merion uh versions how do you pick which one that you should use for a given task good question Paige and yes um thanks for the community for asking this and just a heads up we'll be covering evaluation of large language models in more detail on day five we even have uh somebody from Deep Mind who'll be who' be presenting their research on it um and um yeah so but diving straight into question so there's uh evaluating large language models isn't an easy task because um it depends on the task you're evaluating and what you mean by evaluation there's of course classical natural language uh metrics like plow rou scores which you can use and this is one of the ways you can do it where you have a reference like a golden truth ground truth when you can see what the llm generated and then you can compare the the generated response with the human curated ground truth and you can see how to what extent it matches the ground Tru response now that is a classical way to do it however um given the complexity of tasks LMS are used then you might not always have such a ground truth and even when you do on certain tasks uh you can for example summarization um there's multiple ways to write a summary and all of them might be valid but the ground troot would be just one such instance so it would not be a valid way to um evaluate such cases as well so what we can also do is use llm as autter rators where you uh where you kind of have an llm evaluate an llm and uh and evaluate how good the response of the uh generated by the uh the model was and you can do this in in a point Vice fashion in a pairwise fashion fa you can just say okay here's the response here's my prompt how well on a score of one to five do I does this response score and why is it why is that the case or in a paer wise fashion you compare to resp responses and say okay um tell me uh for this prompt which one is the better response and it says okay response the left one is better because uh whatever reasoning and we actually have that available so you can use that right away and our Vortex Suite of uh through our Vortex AI generative AI evaluation service you can try uh llm uh based aerator evaluation as well as um computational metrics like Rouge and blow which use a reference point can try that out today absolutely and I've I've also seen some pretty impressive open- Source libraries uh doing autora rating and evaluations as well um have been really delighted with prompt Fu which also has the the Gemini API support uh and and some of the other uh some of the other offerings so great to know that there are vertex AI options as well as open source libraries um to to go about doing these evaluations whether they're human rate or Auto evaluated very cool excellent next question from Red 365 um Can someone please explain why for the First Chain of Thought prompt it's also explaining step by step instead of answering that directly Lee I see you nodding um do you want to go ahead and take this question yeah yeah I can uh talk a little bit about that because I had similar uh yeah examples in in the white paper that I wrote and um so first of all for the folks on the call like chain of thoughts Chain of Thought of C prompting that's a prompting technique where you you you try to let the llm generate intermediate steps and uh yeah typically you can do that by providing examples on how to reason and then you don't take the example as what's in the Cod laab you try to come up with another example that which has similar reasoning techniques and uh yeah then you see it will then showcase the steps now this particular question and and thank you everyone for yeah participating in the collab uh I saw some people saying like hey because in the question you had to first type in an example where we did do not use cod uh prompting and then afterwards we will show an example an example where we use Chain of Thought prompting so you could see the difference but the problem was that some people already saw the the the intermediate steps in the first uh first example while it shouldn't it's very difficult nowadays with models that get so good you know our our models are so good like we cannot showcase these flaws anymore but uh you might be able to uh reproduce it by either like often maybe uh just resubmitting the the question uh it might help also just to change the the top K top P or temperature so you get more variance in the in the outputs and uh I saw also in the in the chat some fol saying that uh switching to the older Gemini models will showcase how you can uh showcase the flaw and then later you can fix it with J of thoughts so that's cool stuff very cool I I love uh I love kind of the detailed description of uh of Chain of Thought in and of itself and then also how you can see it expressed as model outputs this is very very cool um and personally I I found especially for things like uh like coding tasks chain of thought prompting or uh or asking the model to explain its reasoning has been very very powerful and has usually led to significantly better results yeah so this is this is very cool yeah um the next question uh from usang Kung is um is something like inam mode um a in a fine tune model designed to return enum values um and Daniel uh it's great to see you by the way welcome back uh uh for folks on the call Daniel and I used to work together at Deep Mind um and now uh now I believe you have started your own company uh uh or you're still in stealth mode for your company so I won't I won't give spoilers um but it's great to have you H would you like to answer this question yeah sure yeah nice to see you as well Paige and yeah thanks for having having me um yeah and and I think that uh in in terms of enam mode I mean it it it very much depends on on what you're trying to do so if you're fine-tuning your model um and you want it to Output enums this depends a lot on how you structure your data so for example if you have three different enum values that you'd like to Output um you would structure your inputs as the question or task that you want to provide to your llm and then the output would be one of the enum values whichever one is most relevant to the question or task um that you asked the llm to provide a output on um and uh so you can find tune llms like that and generate your prepare your data sets with these inputs as being the task or questions and the outputs being the enums um and also you know llms are also sometimes very good out of the box in dealing with enums so you can also say to an llm something like these are the different types of classifications that you can choose or enums that you can choose um choose the appropriate one for the question that we're asking so suggestion would be to also try it zero shot and if not you can create These Fine tuned data sets um and I think there was a question about uh related to notebook uh llm that was uh along these lines page oh yes I I I believe the the question that you were referring to was um from gavas um which is all about uh all about notebook LM how it was built how it was uh how it was created I've really really loved experimenting with it as well as the illuminate kind of automatic podcast generator for any arbitrary data types um the uh I for Noak LM interestingly enough um and this was shared by Josh Woodward at a recent event um it's not using a fine-tuned version of Gemini um it's just using Gemini 1.5 Pro and 1.5 flash um with uh with the addition of some special techniques um and some Secret Sauce especially around retrieval and around uh kind of careful prompting and design of the system um so so notebook LM is not relying on a dedicated fine-tuned example of Gemini it's just using the same Gemini 1.5 models that folks are experimenting with for your assignments um so that's a a great way to get inspired um in that you know the tools that you're building with are the same tools that folks are using to enrich AI applications at Google um which which is which is very cool um beauti um just just wanted to point out uh we noticed there is some freezes sometimes happening from the community on the uh on the live stream um and uh we also be we also got feedback that you liked us to cover some of the course like white paper overview so we'll be covering that later in the call as well and uh let us