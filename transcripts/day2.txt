so first of all um for Allan and for Yan tell me a little bit about Vector databases and embeddings what are they and why are they useful sure um I will start with um an introduction of embeddings and then I'll hand over to alen to talk about Vector databases so embeddings uh in numerical vectors we use these uh eding models eding models are machine learned models that convert Real World objects such as text image and video to embeddings and these models are trained so that the geometric distance or the similarity of the embeddings reflect the real world similar similarity of the object that they represent ining models can be used in recommendation systems semantic search classification Rag and many other applications Google's ver a platform offers two classes of pre-train eding models texting eding models and multimodel embedding models Tex embedding models allows you to convert plain Tex to embeddings while multimodel ones they work with image audio and video input and in addition to textual input so to use these models you simply enable vertx AI in your gcp project and send request to vertx endpoint we also support customizing these models your using your own data set Vector database are storage solutions that specialize in managing these embedding vectors U now I hand I'll hand over to Alan to talk more about those uh Vector databases thanks Yan my name is Alan I'm part of Google Cloud databases and the lead for Alloy DB semantic search and now that we know Vector embeddings captured the semantic meaning of unstructured data Vector search is the operation that given a set of stored embeddings and a query embedding finds the nearest neighbors to that query that is the most similar items to that query Vector search can be done by Computing the distance between the query against every stored embedding and this is called exact nearest neighbor search but imagine how slow this operation would be if there are a billion stored vector vectors that uh uh we need to compute the distance for so Vector databases speed up Vector search through approximate nearest neighbor search which gives highly accurate but approximate answers while doing orders of magnitude less work and there are a number of approximate nearest neighbor algorithms in the industry and they typically fall into two broad categories graphbase the most popular being hnsw and tree Bas and one state of the art treebase algorithm is Google scan algorithm and this is based on 12 plus years of Google research and it's used internally inside Google at Google scale in products like Google search YouTube ads photos and we've Incorporated the scan algorithm natively in multiple Google Cloud products including alloy DB Cloud spanner or manage MySQL big query and vertex Vector search awesome so it sounds like uh embeddings in Vector databases are very quick ways for you to understand how similar certain kinds of data might be to one another um and we have many different manage service options um for embeddings that are offered through Google Cloud also through the Gemini developer apis as well as you can uh kind of use open source tools like chroma uh and other Vector databases in order to to do the work and to store or um to store all of the embeddings that you create for your projects um awesome thank you thank you so much uh and then next question um what are the tradeoffs uh speaking of Open Source Vector DBS um what are the trade-offs between using an open source Vector database um versus a proprietary one like some of the some of the manag services that were just being mentioned on on Google Cloud um om do you want to do you want to go ahead and take that question yes thank you uh so hi everyone my name is Omid and I'm part of um Google Cloud's big query where I lead the AIML and search areas so uh this is a great question so I'd say the tradeoffs are similar to how the trade-offs are in other other areas of technology so to be more specific so the open source Vector databases maybe more favorable for uh aspects such as cost flexibility customizability community support and also potentially avoiding vendor locking um however U often times that needs to be balanced with uh things such as higher maintenance cost uh higher management costs and complexity and potentials for fragmentation down the road and along the way kind of limited Support options now with the proprietary vector databases is kind of the essentially the flip side of that right so they often have an edge in terms of es of use as a more manage service support and more advanced features and stability but the downsides can be cost potential for vendor lock in in some cases and um limited customization flexibility and sometimes transparency now um this being said I would also actually like to uh draw your attention to the increasingly powerful U Vector search and indexing capabilities that are being added across various uh general purpose databases and data analytics and Warehouse platforms and these are being added as first class citizens as Allan mentioned so um you know basically and by the way this is this spans both open source and propriety proprietary worlds but you know for example you know you can think of the PG Vector extensions for postgress and in the case of uh Google Cloud you know for um database on Dat warehouses we have alloy DB spanner and big query all of them adding very strong capabilities as first class citizens so um now the advantage of looking into this option is that you know they would U help aoid void data duplication across two databases so you'll have just one database as source of Truth and you'll continue to benefit from Decades of work that have gone into building these highly Advanced capabilities into these General P purpose databases and data warehouses and essentially you use them as a single source of Truth and they you know you you know you use the added Ai and Vector capabilities which you know keep growing very fast so that's also another option I wanted to bring up as as part of the answer to this question yeah and that's that's amazing to hear that instead of having for uh instead of users having to figure out should I use a vector database or should I use a more traditional database um uh these these kinds of features are being added uh and cross-pollinated across both so you don't have to you don't have to have data duplication and you don't have to understand uh you know which kind of tool or technique you should be using you can just kind of adopt those features Within and things like postgress which most folks already have experience using very very cool thank you uh that that's great to hear um and then also uh so next question um uh this is uh this is for of tar uh do you think new features and capabilities like Gemini's longer context Windows which are now up to 2 million tokens uh in context and Google's recently released search grounding will reduce the need for Vector databases or these uh or these kind of features and and um tools complimentary to each other thank you Paige uh hi I'm ifar I'm from Google Deep Mind and uh thank you for this great question uh I will start by expanding the question and explaining a little bit so there are two new technologies that are mentioned one is long context Windows as Gemini and other language models can support more and more tokens as part of the input and the other technology is search grounding where we recently released the uh capability to issue a web search and ground any answer and validate if that is correct or incorrect so uh first let's talk about the longer context window of course it's really exciting that we can scale to much longer context but still uh we are not ready to actually do that at a very large scale for several reason first uh our we currently support a few million tokens but most realistic databases or Corpus they're much larger we often need billions or trillions of tokens also it's really computationally expensive to do like um full run the full llm over a massive amount of tokens whereas as uh Yan and Allan explained Vector DBS are extremely efficient so they can retrieve from billions of items very quickly and finally we saw that as we put more and more uh relevant content in the context then the lm's reasoning capability starts to degrade so it's often more useful for us to first use the vector DB and retrieve the most relevant documents or content and then perform reasoning on thing which are actually relevant to user question so in that way actually Vector DBS are a really exciting and uh complimentary technique to the new long context uh language model capabilities because now the initial retrieval stage that doesn't have to be uh extremely precise instead we can focus on the recall retrieve lots of relevant documents put that in context and then llm can reason over that large amount of context absolutely I I I just wanted to to say really quickly that I love that uh I love that pattern that you just described of being able to use Vector databases and retrieval to figure out what is the most effective information that should go into the context window in order to help support your outputs um I think I think that's something that's been underexplored to date and I I really love that you called it out yeah yeah I'm super excited and also it's helping us to think a bit differently in the past we would like have very limited context so we would focus too much on the uh Precision of the retrieval whereas now we can be more recall oriented because we can support slightly more amount of tokens in the context but then through a larger net get all the relevant content and then reason over it uh there is another part of the question about Serge grounding I would just very briefly mention that Serge grounding allows us to ground uh to the web the all the public information but often we want to do search or do grounding on our private information like our personal data or in a company's proprietary private Corpus and for that that we still need Vector databases because public search will not have access to those private data yeah absolutely and I and I know that Google cloud has uh has implemented a feature that is um not just search grounding but also grounding on arbitrary data that might be hosted in GCS um so so I think that I I think that you're right you know there will always be a place for both uh you know retrieval to get the most pertinent information and then also the ability to to kind of search um search over the web to to be able to to find the the most recent information cool um so next question what are the fundamental challenges and opportunities for Vector databases Alan would you like to take this one sure thanks paig so uh speaking from the perspective of building a vector search index natively within an operational database um so uh from an operational database uh customers perspective um uh this is a system of record uh for these users where they expect transactional semantics strong consistency fast transactional operations and data that spans memory and disk one interesting challenge that uh We've uh been working on has been how to achieve the same performance of uh specialized Vector database systems that don't have the same constraints U as one example of an innovation the allo DB team has implemented a custom underlying buffer page format in order to leverage scans low-level optimizations and other opportunities that I see that apply to Vector databases in general uh are in the areas of improving usability and performance when Vector search is only one part of the application logic uh so for example uh Vector search with filtering is a well-known challenge where it may be more efficient to do post-filtering or pre-filtering operation uh depending on the filtering condition and the user's intent is clear return the nearest neighbors to me that passes the filtering condition um right now a lot of the systems uh uh need the user to specify okay I want to apply the filter after the vector search or vice versa uh but uh the vector search uh databases should be able to automatically figure this out um uh figure out the actual filtering operation that will give the best performance uh without the user needing to specify that and this extends out to combining with other application logic like joins aggregation and text search I love I love what you just called out about introducing these AI capabilities into traditional databases and then also improving the developer experience for them I think that that will bring um a a lot more capabilities to folks without uh necessarily making folks learn how to use uh use new tools or techniques um and I'm really excited to see uh to see what everybody builds as well as the the new improvements that are coming into products and and G CP into open source databases so this is very very cool um thank you for thank you for that great answer and next question so so we've been talking a lot about uh you know what happens when everything goes right when we're able to find the most relevant documents or or kind of data assets uh and then use them as part of our projects either by uh by taking in the information and using it with the extended context windows or or by using it directly within the databases um what happens if uh retrieval uh uh if retrieval augmented generation doesn't retrieve your relevant documents or your relevant data assets what can you do um and shachi do you want to do you want to start oh yes uh thanks page y so it's uh it's not uncommon that uh R systems when they retrieve the document for specific query if the document previously is not like embedded in the Corp or the corporate doesn't have relevant informations some irrelevant documents may be retrieved as the top neighbors for the query and uh so in that case I think it's rely very heavily on the like the backand larger models of factuality capability to understand whether the retrieve documents can or cannot answer the question and there are like other methodologies we can uh make sure like the rack systems can can still function well when the non-relevant documents are retrieved so I will pass to an end to introduce about the agent yes thank you Zaki so yes as zi mentioned um it's um you can tune embeddings like you can kind of make sure that the model can understand and um the rag system works well because when the search part works well of rag the the retrieval then um you can improve the generation as well if the relevant documents are improved um but uh what you can also do is uh um uh make instead of just doing uh simple rag system where you kind of use embeddings and then Vector databases to find the most relevant set of documents and then uh like five or 10 or so such documents and then using those documents in your prompt for Generation you can use an agentic system uh where you uh an AI agent kind of figures out okay based on the query and the prompt uh it calls various it takes various different steps it can uh call Vector databases different ones or it can also um uh take different steps such as use grounding services and various different tools and techniques in order to retrieve the most relevant documents and then Supply that so instead of a static process of go ahead find me the top five most relevant documents and feedback it figures it's a dynamic process where it takes the steps tools uses a bunch of tools and technology and figures out how to uh how to um provide the most relevant content for the prompt so we by the way um uh on the next day day three we'll be covering the topic of agents in in more detail so I'm super excited about this excellent so it sounds like there are many different there are many different things that people can do in order to kind of improve the likelihood of getting the right answer the most relevant answers for for their rag approaches um so this is great and I am I'm sure that a lot of people are excited about day number 3's content and getting to be Hands-On with creating their own agents and using them to solve problems um so see uh excited to talk about that tomorrow and then next question um the mainstream of llms are decoder models um how can we train an embedding model from a decoder only model backbone Umar do you want to take this one as well good um yeah this question is a bit uh technical but it's also very exciting and interesting um so typically the traditional large language models they are decoder only and what we mean by decoder only is that they are trained in a way that the task is to predict always the next W or the next token and the model can only look at all those w or tokens that came before it so it cannot look at ahead of the token like say words that come after the token so this is like a unidirectional left to right causal attention and this is very efficient uh we can do lots of optimization during training assuming that this attention is just one directional and we don't have to look ahead and as a result this is being adopted more and more for llm training however for encoder style task for example if we want to create an embedding for a piece of text it actually helps us if we can have like bir directional attention that means every word can look at all the words that came before it and all the words that came after it and for this actually if you look at the state-ofthe-art most recent embedding models they are all starts by initializing from decoder only large language models but then they are further ad adapted or more trained to learn to do bir directional attention that means pay attention both to words before it and the words after it and that improves the embedding model quality so in short actually uh uh the state-of the-art embedding models do use bir directional attention something like encoder style models but they are usually initialized from the decoder only backbones and then some additional training happens to make them really strong coders excellent thank you for that great description and uh and if folks want to learn more about this where would be the best places for them to go look do you have any favorite papers or textbooks that they could go that they could go to learn more uh I think of course there's the classic paper about Transformers that explains the attention really well and the B paper which shows the advantage of bidirectional attention of a transformer based encoder uh but there are also a few recent papers one I can mention uh from Nvidia uh where they explain in depth how they are adopting decoder only models for encoders but there are many other similar work both inside Google and externally excellent and we can make sure to add those paper recommendations to the to the YouTube video um for folks to be able to to go and to take a look at um thank you for thank you for that great discussion and then our final question before we head into the pop quiz so hopefully everybody has been paying attention um uh Jun huk uh after all of this discussion um does this mean that conventional methods for creating embeddings will become obsolete um you know I I think we were discussing a little bit around uh kind of these multimodal techniques um do you have any uh any perspective on whether conventional method um might need to change or evolve based on all of the all of the questions and all of the insights that we've been discussing here today yeah uh so my name is chin Le uh so there's a small typo it's not you but j i but anyway uh I think the conventional method uh if you're talking about the OCR related technique where you extract the text first from like a PDF and then converting into a like a text edting model uh I think there's a good chance that uh the future embedding models that recognizes like text from from an image might be able to capture that uh into the Spector representations uh but I think the current status of the multimodal embeding are not accurate enough to capture all the uh like the the exact text in the image image so I would say uh for for maybe next year or so we might need to uh combine both methods to take the advantages from the fors excellent and I apologize for the typo um uh if folks want to to learn more about Jin Hook's research um I I um we will make sure to add um kind of a a link to Google Scholar um as well uh in the in the YouTube notes awesome and now um without further Ado um we are going into Pop Quiz territory um so the pop quiz is just uh five or six questions after the end of every day to kind of test your comprehension both of the podcasts the papers that you've been reading as well as the Q&A discussion that we've had with all of our uh all of our subject matter experts here today um so get out your pens your pencils um just something to write down uh write down your answers and we'll be walking through each one of the questions um uh in real time so uh so make sure to to get these uh to get these questions um answered first off um what modalities can be converted into embeddings um we discussed this a little bit in the in the Q&A session and you should have read a little bit about it in the white papers as well um for modalities that can be inverted uh converted into embeddings um a image B video C text or D all of the above all of the above could be turned into into embeddings um I'm going to give you a few moments to to pick your answer and count down 5 4 3 2 1 hopefully everybody picked D all of the above um you know as we were discussing before we can have text only embeddings but we can also have multimodal embeddings that Encompass image video text audio uh kind of all of the modalities that you can think of um so uh so definitely explore embeddings not just for text but also for other use cases question number two um which of the following is a major advantage of scan over other approximate nearest neighbor search algorithms we discussed this a little bit in the Q&A especially uh during the the questions to Allan and Tion about uh about Vector DBS and and databases um so so uh get out your pens and your pencils uh and think about the think about the best answer um number a is uh it is open source and widely available B it is designed for high dimensional data and has excellent speed accuracy trade-offs C scan only returns exact matches or D it is based on a simple hashing technique and has low computational overhead which is the best answer um and again I'm going to do a countdown backwards from five five 4 3 2 1 hopefully everybody picked B um scan is designed for high dimensional data and has excellent speed accuracy trade-offs that's part of the reason why um we care so deeply about it at Google um because we we need to care about things being fast and being scalable um so uh so hopefully everybody got the question number two correct um number three what are some of the major weaknesses of bag of words models for generating document embeddings um this hopefully you you got to see a little bit in the white paper as well as maybe in some of the code Labs um but uh value a they ignore word ordering and semantic meetings B they are computationally expensive and require large amounts of data C they can't be used for semantic search or topic Discovery or D they're only effective for short documents and fail to capture long range dependencies what are some of the major weaknesses um of bag of words models um I feel like I should have Jeopardy music but five uh Four 3 2 1 hopefully everybody picked a they ignore word ordering and semantic meanings um then next question which of the following is a common challenge when using embeddings for search and how can it be uh how can it be addressed um is it that embeddings cannot handle large data sets and you should use a smaller data set in order to address it is it that embeddings are always superior to traditional search and there's no need to address that challenge um is it that embeddings might not capture literal information well um in which case you need to augment and combine with full text search or is it that embeddings change too frequently and so you should just try to prevent updates um which of the following is the is the correct answer um think about it in five four 4 3 2 and one um it's that embeddings might not capture literal information very well um and so if you do use it and combine it with full text search um like some of the techniques that we were discussing today um then you will be able to get much more accurate results and and hopefully be able to build stronger and more robust systems and then final question um what is the primary advantage of using locality sensitive hashing for Vector search um is it that a it guarantees finding the exact nearest neighbors B that it reduces the search Space by grouping similar items into hash buckets C is it that the only method that works for high-dimensional vectors or D is it that it always provides the best trade-off between speed and accuracy what is the primary advantage of using locality sensitive hashing um hopefully everybody was paying attention in the white papers um and 5 4 3 2 1 um and it is that it reduces the search Space by grouping similar items into hash buckets