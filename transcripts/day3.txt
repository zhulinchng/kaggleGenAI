first off Stephen uh welcome to the generative AI intensive course we're really excited to have you here and also congratulations on winning I believe it's one of times products of the year for notbook LM is that correct yes uh one of times's in inventions of the year uh we very excited we did that just happen a couple of weeks ago that's been really fun excellent and you are also at Ted aai San Francisco I believe as as one of the uh one the guest speakers kind of articulating some of the ways that Noak LM has been impactful and kind of driving change um in the in the generative AI space I love how it's created kind of a new paradigm that isn't just the the regular old chatbot Paradigm um for for people to interact with these models yeah that's part of the vision from from the very start I me we've been working on what became notebook LM for more than two years now and yeah the idea was like what would happen if you if you started from scratch um to build a a tool for thinking and organizing your ideas and organizing your research and and potentially writing if you knew from the beginning that there was going to be a state-of-the-art language model at the center of the product like not not not adding it to an existing product but like start from scratch with the idea of the whole things that we built around this this incredible new technology and and so that's that's what we ended up building and we kind of the way we've kind of come to describe it is it's basically a tool for understanding things um and everything in the product is designed to help you understand complex material organize ideas extract Insight see patterns and from the beginning it was predicated on the idea of what we call Source grounding which is sometimes called rag um in the industry we we think Source grounding is a nicer term for it but it means that you upload the sources the information the documents that you need to do your work um and then from that point on all your interactions with the model are grounded in the information in those documents and so we go through and we have things like inline citations so when you get an answer from the model you see all those answers have little kind of footnotes and you can click directly on those footnotes and see the original passage in in in your sources that the model used to come up with that answer which means that you can fact check you can dive deeper and read the original Source material um it's just a really great way to navigate through information that was just not possible before and all of this is is depend on this incredible model of Gemini Pro 1.5 uh both the ability to have that long context where we can put all your documents in the context window of the model um and also things like citations which em and I does kind of natively awesome I I love being able to to understand uh like very discreetly how longer context can really enable these features for creators um and and I know many folks uh many of the kind of developers that I've talked to to have mentioned that they get frustrated sometimes where it can be really challenging to figure out if a if a large language model is hallucinating something or if it's actually referring back to content um so having having those citations baked in is is really compelling um excellent so so tell me also a little bit about what has been your favorite use case for Notebook LM that you've seen um either within Google or outside of Google yeah I mean the thing that has been amazing for me personally is I have a single notebook um where I I have collected as sources 8,000 quotes from books that I've read over the years and articles that I've read over the years that have been kind of part of my research and things like that and plus I I write books in my non-google life and so I also have the text of a lot of the books that I've written in a single notebook so you can have like 25 million words worth of sources in a single notebook and so when I when I go to this notebook that has all these quotes and has all this material that I've written I can just you know kind of Riff on ideas or ask questions and the The Notebook the AI That's in that notebook basically has an incredible expertise in the history of my thinking like 20 years of my thinking is kind of active in that notebook and so whenever I'm like wrestling with a new idea I go to that notebook and I'm like he what do you think of this any any Sparks any connections and it'll be like oh yeah you read a book about this like seven years ago that you've totally forgotten Steve and then here's the exerpts from it or here's what's relevant to to it you know so it feels like an extension of of my brain in this way that is really powerful so that's that's been really exciting and then we can talk about you know we released this new audio overviews feature uh that came out um two months ago or so and that's where you upload your sources um and you generate a kind of remarkably lifelike podcast style conversation between Two Hosts there's another version of this at Google called illuminate which is I believe what's being used to generate the podcasts for um for this for this course um and people are just loving that feature and the craziest use case for that is that we that we've seen a lot of people do is you upload your resume and you generate a an audio overview based on your resume and the and the hosts are basically instructed to be engaging and enthusiastic about whatever you give them and so it's a great way to feel good about yourself because you'll upload your resume and be like wow Stephen you really had a very busy year you've accomplished so many things excellent I love uh I love having that personal that personal hype uh hype uh notebook to to be uh to be celebrating all of the great successes this is wonderful and I the the idea that you mentioned around having kind of like a a thought partner um that uh that's basically that's basically you just you with perfect uh Perfect Memory like perfect recall for all of the the different bits of information that you've experienced over the last years that's super super cool um and really really excited to play more uh if folks are are interested in in testing out notebook LM uh what is the URL that they should go to notebook lm. goole.com excellent thank you so much and speaking of having kind of a a personalized uh a personalized hyper expert uh sort of companion for doing work um Wes uh I have been really really uh enchanted with the AI features that you and your team have been baking into collab I use it literally every day both for code generation as well as fixing errors and explaining errors um so thank you thank you so much for your work and then welcome also to the to the kaggle generative AI intensive course thank you yeah we use it we use it every day too as we build out moreper features um collab and and data science agent is uh just been really a benefit for all of us one thing to kind of note is that the data science agent for collab that we announced back in May at IO um was uh is just the latest in a string of AI kind of enhancements to collab and all of these things really kind of do the thing same thing which is that they increase the velocity that all of us can uh move at when using AI tools um well and doing development in collab other places with more velocity of course means we can experiment faster try out more ideas and we found as we're doing our own kind of AI ML and data science work that the that the primary determining Factor about how much we can do and how much progress we make is the velocity of the experimentation we can have and so enhancing people's ability to to have more velocity by giving them tools like the AI features um is really kind of magnifies what they can do and specifically for the data science agent uh what we back in in May was that uh is that you can kind of ask a question give a data set or even ask like a modeling question and uh of the agent it'll go off into a multi-step process including things like importing the data exploring the data cleaning the data visualizing it and ultimately ending up with qu answers to questions as well as um models that kind of thing as the output then from there you can iterate and kind of go on to do bigger and better things we're contining to enhance this and and bring out new ways for people to interact excellent and and I'm assuming behind the scenes these agents are using tools like function calling um and and all of the great kind of uh the great features that we learned about during the the course of this last lesson is that correct that is absolutely correct we use function calling use retrieval all kinds of wonderful things that we've been talking about excellent that's that's perfect uh and and I am very very excited to hear that more and more AI features are going to be coming to collab I know that is uh increased my velocity personally uh uh you know magnitudes over the course of the last six months uh so I excited to see the the next features get released um and the next question would be for both Stephen and Wes uh we already touched on this a little bit but uh how are you thinking about function calling and retrieval and the systems that you're building are these techniques and tools that that are being used it sounds like they are for data science agents um Ste are for is retrieval getting used within NOA km is that is that one of the things that that's kind of baked in to the to the citations process or the the data recalling process uh yeah I mean it's a little bit above my technical pay grade to get into the details of this but but yes we do I mean we do this kind of semantic retrieval um that's an important part of the process um particularly if you have a large set of sources that exceed the amount of information that can fit in the models context and so there's a you know we've we've built some quite smart tools for kind of getting the most relevant passages from your sources um and then putting them in front of the model with the custom prompt that we have um and so that's that's been something that was literally like the first feature we had um back in the day it kind of it it was just in a collab basically in in in August of 2022 um you could give it some Source material and ask some questions but the source material could be like a page long instead of you know 25 million words so we made a little bit of progress amazing I I love that the the kind of fundamental Lego brick uh building blocks that that folks are learning this week are are the same tools and techniques that folks are using to build the products at Google and then also external to Google um this is very exciting thank you thank you both uh uh for for the great discussion uh and for uh for building the the products that that y'all are sharing um we'll have links in the the YouTube video as well for folks if they want to go and try out uh try out notebook LM and then also try out the AI assisted features in collab um next question uh this is for Patrick and Julia have you seen any major changes to agent compositions and production since you started writing the white paper um and I and thank you also for authoring the white papers that folks have been reading um over the course of this last day what's changed what's stayed the same and then what are the challenges and opportunities you see kind of the new exciting things coming down the road yeah for sure uh I'll start off on that one so you know thanks for having us today you know it's it's interesting you know Julie and I have been thinking about agents for a really really long time you know uh way before the the geni you know kind of craze took took foot in the world you know Julia was working on things like cognitive architectures and and trying to you know do research on that and understand how could we Implement these things in production and and I was also working on sort of like the the precursor to generative AI agents and just the conversational AI space and natural language processing and stuff like that so when we started writing this paper you know it's obviously it's really hard to predict the future in this space and know like what's going to come in the next three months or six months because in the geni space that's kind of like an eternity right um so as we thought about you know putting you know words to paper and and thinking about what are the ways that people will start kind of architecting these agents in the future we thought really carefully about like what are the composition of Agents going to look like what's you know what's going to happen a month from now what's going to happen 6 months from now you know things like that and so I think the things that have stayed the same uh roughly are the architectures you know we see that you know agents have models agents at the foundation you know they have tools that they have access to and then these you know kind of orchestration layers run times uh memory uh goals you know to try to achieve these particular things so a lot of that has really stayed the same and and I think we're happy to see that the paper has aged well as as we've kind of written it and has as things have you know grown over time um the things that have really changed I think are the fact that the models themselves are just simply getting better and better over time you know when we first started writing the paper we had some really early llms and you know they were okayish but it ended up you know you had to put a lot more into your reasoning layer and your logic layer to kind of control the things that you wanted your model to to do uh but with some of the latest models that are coming out it really just makes the orchestration layer so much more simple uh a lot of these models are baked in with much better tool calling uh much better logic and reasoning uh abilities to do Chain of Thought natively abilities to do you know code execution natively and so all of that is really just making um sort of like the higher level abstractions of building these agent Frameworks even you know easier and easier over time so I don't know Julia if you want to add anything to that yeah absolutely um so big plus one to everything that you know Patrick has said we've really been thinking about this for a while and um I'm loving hearing all the shout outs to um function calling into tool use today um because that's you know been something I've been thinking about for for quite a long time so really excited also to see what folks start to build with uh the contents of the of this course and I think that's a little bit what I'd love to focus on here on on the challenges and opportuni side that we see I think um there's still so much that can happen here in this space right now you know we still see I would say them I would call them fairly simple implementations often with rag with a handful of apis things like that and what I'm starting to see and what I'm really excited about is the ability to solve kind of complex real world problems with lots of different apis with mutating actions with running different simulations Etc and I think there's just so many industries that this can apply to and also that it goes far beyond the realm of just chat implementations per se and and there's so much more that you can you can start to do with these things if you think about Industries like supply chain or security or kind of other spaces that um that really have kind of background processes that can also work and work and automate and and when you hear things uh like what notebook gum is thinking about also going outside of that kind of uh frame and interaction awesome thank you both so much and thank you for the great work that you've been doing to build robust function calling in tools used into the the vertex AI Gemini apis I know we've also been drawing on some of those learnings for the Gemini developer apis um so thank you uh thank you for making it real um and Julia has anyone uh you know speaking of specialized domains and specialized kind of versions of function calling and tools use has anyone implemented an agent focused on security um uh and uh where can I find uh overall guidance on implementing agents for cyber security that's a great question and as I just mentioned the security industry is a space where there's a lot of opportunities for agents I mean just think about things like threat classification tracking um Etc and I really think that a lot of this is just at the beginning so as far as I know we'll actually be talking about secm tomorrow so look forward to tomorrow's sessions where we'll look into I think a semi-auto semi agentic setup um that can address some of those security challenges using using llms so more to come on that front excellent excited to have the discussion tomorrow um and also is there a way to evaluate the accuracy of the tools that Gemini selects um if there are de uh deviations whenever Gemini uh selects a tool are there any ways to remediate um and I I've run into this at least a couple of times where um when I when I specify different functions or or different tools to call um sometimes multiple tools might be applicable to the same problem um how can you start thinking about uh doing a kind of composite tool selection or or just uh getting additional information from the user before selecting a tool yeah absolutely and I absolutely love this question because and it's something that Patrick and I spent a lot of time thinking about also as we wrote the paper I think there's an entire followup section to do on kind of um evaluation and taking these things from kind of testing to uh to kind of production and and and iterating on each of these these tools um from that perspective right now I'd love to kind of just do a quick shout out to actually some of the vertex AI tooling that we have around evaluation so anybody that's working kind of with function calling um you can actually check out the Gen evaluation service and using kind of your prototype you can upload data and actually test kind of um and evaluate outcomes to kind of get a sense of of the accuracy and those kind of things ahead of time and start to see how you can kind of fiddle with the with the different knobs and dials in order to kind of make these things more accurate so I think um folks will will follow up with a link on on that front with regards to function calling now um like specifically I think that there are a couple things that are pretty generic to gen as a whole um you want to log and analyze kind of tool selection decisions over time so you want to track giz choices for tools their user queries and then understand kind of the rationale and the patterns behind those things so that you can start to work and and improve those kind of potential errors and um and then at the same time that ties directly into developing kind of those test cases with a diverse set of queries so as with any gen product you really want to kind of get those use cases get those testers get those people that are are starting to use whatever you're building so you can see how they're using it and and collect where Gemini is also failing or where there's kind of um issues with the with the tool call so that you can then um use those things to over time do things like refine the The Prompt engineering we've had all different kinds of examples where um you you have to be really careful with the wording that you use in in the prompt that you're putting inside right we we've we've seen um seen examples where essentially without meaning to um we actually told the model not to use functions and um and um and so we we we just have to be really really careful and by like collecting those examples we can then actually look for those patterns and look to understand kind of what the the model is doing um secondly we can obviously work on expanding and continuing to improve the tool definitions so ensuring that they're really unambiguous and actually often using the model to help you um help you do these things is is is really useful and um over time you can also as you especially think about getting to production you can uh also think about things like um doing um targeted learning where you're kind of fine-tuning Gemini or really um um giving it uh a rag based query on a tool example ahead of time uh so that it knows a bit better as to what it should be doing and in in in this case so I think there's there's there's definitely some opportunities there um and it's going to continue to kind of improve as as the models also continue to improve yeah I love that and I I really do uh feel like you can you can get a lot of uh you can get a lot of alpha just by investing a lot in prompt engineering at least today um and helping the uh uh using gemini or using other models to help build the prompt um to to have more robust systems um if folks are are interested in open source options for uh for evaluations as well um I I've been really loving using prompt Fu which is an open- Source uh evaluations option um that supports both text code and and multimodal based evaluation so we'll have the link for that in the in the chat as well um excellent and then sorry sorry paig I just G to jump in real quick and add a comment about evaluations um if people are watching this and you're curious more about evaluations we're going to be covering on day five in our mlop section of the same course so join us in a couple of days and we'll we'll touch more on evaluations as well excellent and uh moving just a little bit because uh one of the fun things about Google offices is that to conserve energy if you don't move enough uh the lights go off in the back of the office so uh if you see me moving around um that is why excellent thank you for calling it out and uh get excited for day number five where we'll be discussing evaluations in more depth um and then next question uh in building an agentic rag system for better responses there are a lot of intermediate steps between the user query and getting a response from the large language model how can we deal with that latency um Patrick and Allen do you want to take this one sure yeah um I mean I think that uh don't forget about traditional application development best practices you can invest in pre-processing improving the quality of the data improving the chunking mechanisms or whatever the details are for your data and then uh improving the quality and performance of the search right because rag or uh grounding is really all about getting the right facts or chunks into the context window of the prompt so you can invest a lot in the system um when you're going to a gentic rag you're basically adding more and more steps perhaps reducing those steps through uh that invest is as sort of standard application uh best practices um Additionally you can do tricks like execute multiple queries in parallel and um you can evaluate the responses of those parallel queries and pick the most relevant answers uh there's some other stuff uh Patrick we've talked about this a bunch do you want to jump in yeah I I think just to add to this you know from a developer perspective there's always this propensity to want to use the the coolest fanciest um you know kind of code or implementations out there but a lot of times you know going back to kind of just basic system design principles and thinking about Simplicity is really the right way to go and I know generative AI right now is super cool and agents are cool and rag is cool uh but sometimes just doing you know retrieval from a cach and responding with that answer is the best actual the best outcome because you have to remember that when you're building these systems for production um what really matters is the outcome right like the results that you're trying to achieve it doesn't matter that you implemented the coolest most agentic system behind the the scenes you know what matters is the outcome to the users of that system and so kind of keep those things in mind as you're doing this you know traditional system design thinking around how you should be implementing this for your your customers your partners or for yourself can I do one more quick plug uh you mentioned Cash A lot of people are talking about caching the final output and that's awesome right um context cash is an unsung hero a lot of people are not experimenting with that enough go check out long context window and context cache uh you can get a lot of stuff done for especially for small use cases or early prototypes um there's a lot of opportunity and those features and in we cover context caching as well as some of the optimization step for retrieval and making llms faster now day one and two so go read those fight papers those of you listening for more information yeah excellent and I love the I love the call out as well to use whatever tool is most appropriate for the task even if it isn't necessarily like the most bleeding edge generative AI tool um I've I've often been using function calling for calling smaller open source fine tunes models um to do specialized things like image segmentation um or or uh kind of more dedicated tasks and given that they're much smaller models they're located on device um they can uh they can get much faster results than if you were if you were using a much more powerful model um and also often for cheaper so so function calling uh it has been a really nice win for helping right size calls to the to the right tools awesome Paige maybe just one quick throw in there we've also seen it with calling like time series forecasting models um which gets really really interesting when you can get kind of a Time series forecast into kind of whatever system you're you're starting to use and things like that so plus one on that front excellent very cool and and so last question um before we get into our Pop Quiz um uh this is for Allan when would you recommend using a minimal implementation like the Gemini API um versus a stateful graph-based approach like using Lane graph um which is an open source library that we were using in the Cod lab sure um this is sort of what we were talking about before these are just design principles right um are you doing a quick prototype uh please just start quick like just get some ideas out there play with the tools learn the tools see what's capable see if your use case works out um the lowest number of dependencies just just get in there and play but uh it is difficult to control agentic systems uh and especially as the complexity grows your level of effort to control that system grows so um I I think maybe day one just use the SDK play around and get going probably day two get uh invested in some kind of a system to control that behavior um I do like Lane graph or graph based representations it's not the only one there's lots of other Frameworks out there that use graph based representations breadboard is a great one um that uses graph as config as opposed to a graph implemented in code both of these are just means of understanding what the agent is capable of doing that's like introspection what might happen and observability what did the agent do what choices what steps did it make Etc these are really tools to help you as a developer understand what's happening and control what's happening that that's all it is so these are these are techniques that have been created to help you as a developer better understand the system that you're building and better control it that's it so I I do think play around first um you don't have to invest in a system but adopting a system on day two is probably a good plan because by day 20 you will wish that you had done so yeah excellent and there and there are many great open-source libraries um being created to help with precisely these uh precisely these problems things like crew Ai and agent Ops and lingraph um I think Gena yep lots and lots of uh lots and lots of things to to help bring uh uh to bring reason to the chaos so uh so excited to see even more of these get built out and become more robust over time um awesome thank you all so much for uh for our experts uh for giving your time for uh your insights and then also for writing all of the great curriculum and white papers that we've been learning from these past uh these past days um really appreciate you and your time uh and thank you thank you