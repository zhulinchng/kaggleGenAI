I'm going to start with our first question which is for UMES and for Scott um tell me a little bit about large language models for security um what are they and why are they useful yeah great um happy to be here um just a quick little bit of background for me I I run the uh data science research team in Google Cloud security that uh builds secm um and I've actually been working in this space of ml applied to security here for going on 20 22 years now so I've seen quite the progression uh LL is definitely like an interesting new area for us um so this question is like very much pertinent and very top of mind for for the entire field honestly um and so you know I think probably the way to start is to think about well like what makes cyber security a little bit different than some of the other sort of typical business areas where an L will be applied um I think the first thing to note is that in cyber security it's not one big monolithic thing there's actually tons of different tasks that people um you know perform on a day-to-day basis to do cyber security work everything from doing binary code analysis and malware analysis to triaging alerts to uh you know doing more devop style administration of your of your network and so um you know the the the tools the data that's being used the sheer diversity of different tools and data um you know really make this kind of interesting in that you don't really get this like overwhelming momentum of data of public data available right um as you would in a lot of other cases for llms um another kind of confounding Factor here is that lot of that data is typically considered to be highly sensitive by organizations right security is normally people's you know right next to health and pii is is you know pretty much right there right there on the list of things that they don't want to have revealed uh publicly uh which again further reduces the the potential for you know what's available publicly for us um and then the last thing is that you know if you think about a lot of the tasks that threat researchers and and malware analysts do they often times Bor like right on the edge of what you might call safety mechanisms for in the more General sense right so if you're doing malware analysis or you're you know trying to you know develop a plan for red teaming your network those seem an awful lot like things that would Tri trigger a safety mechanism and they often do and so the combination of all these things taken together means that you know it's really hard for a generalist model to really have an expert level of understanding and reasoning about all of this variety of data such that it really covers all these customer needs that that you might have um so that kind of is the motivation behind it you know you know what it is specifically you can kind of think about starting at one of these foundational models um but we have a continued pre-training task where we kind of feed it more special specialized expert level books and uh documents about cyber security to just kind of build its vocabulary in that space um and then we have a bunch of very specific um fine-tuning tasks so we do full parameter fine tuning um but these instruction tuning type of tasks are very much pointed at these areas where the generalist model is unlikely to have seen or be exposed to things right so like for instance analyzing very long form security alerts right and really truly understanding uh you know not just from inferring what the the keys and values are but in actually truly understanding what's happening in the alert um once that's done we have an human alignment process which again is a really interesting thought because when you're you have such a diverse uh area as cyber security there's lots of different humans with lots of different interests in in what they want the model to actually be providing to them so like a threat analyst might be different than a security uh operations analyst um and then finally we we often leverage uh fine tuning as a not had mentioned uh you know use Laura based fine tuning to do um basically specialization towards either specific customer environments or you know tasks that maybe are particularly out of distribution for the the core models and kind of need a little more extra reinforcement somehow um you know I think they're particularly useful because frankly even though the generalist models might be able to get you there you know they might be able to provide an answer in a lot of cases um the rate of hallucination for those answers tends to be higher uh than if you were just to use a specialized model and in some cases it just simply cannot actually perform tasks at all in any sort of reasonable way like we have you know a number of specialized uh domain specific languages uh query languages that you know is very very important to pull new data using and um unfortunately because they're poorly represented in sort of the public internet the models simply don't really know what to do right like often times will just spit out SQL but you can't use SQL in these instances you have to at the the specialized language so um that's I think uh sort of the the what what and the why of of using it um I will say here though that one last bit is that um you know I think a lot of people think of llms as like memorization machines like let's pack as much information into it as we can and that's actually the opposite of what our thought process is we actually are very much focused on it performing tasks on data rather than it memorizing something and the reason for that is you know as an not mentioned in his intro was you know threats evolve on a daily basis so you could easily imagine how this is going to totally blow up in our face if we try to like give it the the most latest information on threats every single day we'll be basically retraining an LL every 24 hours in order to try to make it relevant and that's clearly not my viable option that's wonderful I love that you call out how uh the pre-training data kind of the the information that the base models are trained on is really really um sort of uh security use cases are underrepresented which means that the model as part of its pre-training process never learns how to do those kinds of tasks or if it does learn it might learn only kind of the most uh sort of remedial 100 101 level um steps as opposed to the to the more detailed like Enterprise grade security kind of challenges that y'all are trying to tackle yeah or or only the most important you know the most well-used or popular tools that are out there and ignoring the very long tale of things that people have in their environment excellent so so next uh for the next question um let's go to uh let's take a a sort of turn to the healthc care space um Chris how are you thinking about benchmarks like Med QA and when will we reach 100% on Med QA thanks paig hi I'm Chris I'm working in Google research on health a um and for as youve seen in the white papers basically a couple of years ago we published our first uh medical model metp which was the first Model to um achieve a passing mark on Med QA and Med QA just for context is basically a multiple choice test that is similar to questions taken by medical students for the license exam it is a few sentences describing a scenario a patient with a couple of lab values or a couple of symptoms and so on and then three to five options on what the right answer is and then just many hundred questions of those and then you get a benchmark overall and the big upside of The Benchmark is that this was attempted for many decades so we have a good comparison across many technology evolutions on how good or not good technology was able to get there like 10% correct 20% correct and so on um and if you look forward to uh a year or two ago it reached the passing bar it reached the expert level bar and if you look like today a couple of models including Gemini for instance are achieving the 90% wage of answers the big downside is it's a little bit of an artificial scenario like the patient information is very compressed um while for instance in reality a patient information is like a full health record of many years and medical images and lab values and so on and secondly it's multiple choice which is easy to compare so it's very good to Avail it automatically but it's not very realistic like most doctors don't have M multiple choice available when they look at the real case so are we going to reach 100% on benchmarks like met QA probably somebody's going to get there um I would attach some questions or whether it memorized how to answer test questions whether how to really reason understand medical knowledge but what I really anticipate is going to happen more more is that we're moving on once a benchmark reached the 90% bar that we're moving on to more sophisticated or benchmarks or benchmarks that are much closer to reality so for instance if you look at the papers of the last year from our group you saw like many more Medical Imaging tasks or you saw like hard case challenges where you see a full patient scenario and you need to come up with a diagnosis or treatment recommendation for the sake of understanding whether the models could do those kind of tasks um so that's my real hope that we're moving on to more more complex benchmarks and sophisticated benchmarks um and I assume met QA yeah somebody will reach 100% but I don't see uh that as a very important Milestone anymore as such yeah I love that you call out that we need better benchmarks especially for these highly highly specialized expert tasks um because to your point the uh multiple choice questions it would be beautiful if life was as simple as a multiple choice question uh answering test but uh it would uh unfortunately you know the world is a little bit more complex for that so so I hope folks listening today are inspired to create better medical benchmarks for large language models um especially uh especially things that are a bit more complex than than multiple choice Q&A awesome so next question this is for Antonio um welcome uh Antonio is one of the the leads in our Google Cloud Office of the CTO um Antonio what are the tradeoffs that you've seen between general purpose and fine-tuned models um or is it that uh fine-tuned models are eventually just going to have all of their knowledge baked into the base models themselves well thanks for the question well perhaps first I introduce myself I work for office of the city an organization in Cloud um and all my life I was working in AI before before working in Cloud I was working in search I went to S to to check when was my first paper in search it was 27 years ago so long long time ago uh a lot of changes in AI in uh in this uh in this period well in the last uh three four years I would say that we saw a number of models always increasing in size and today the situation is that we have models that are very large and able to deal with complex uh ques very complex prompts for instance Gemini Pro is such a model and we have models that are more efficient uh still able to deal with a complex queries but more efficient from the point of view of latency from the point of view of cost and so there is a need to balance and take tradeoffs in in in these models and people people are normally looking at you know when they trade make these tradeoffs they look at different dimensions including for instance the quality of the answer the cost of serving the answer the speed I mean the latency and there is a a need to very frequented to simultaneously optimize this uh this uh multiple Dimensions so the reason why there is an interest in specific domain models is because you canot uh have some some good good gains with model are smaller but specialized in in particular in particular domains and so for instance uh security and Medicine are two examples but there are many other examples the good news is that today U there are broad range of tools that we can use to do to do this um you can start with fine tuning there were discussions about fine tuning earlier and today and also in the previous days where you essentially take a model that is already TR then you adapt this model to a specific domain providing some sample in the specific domain this is if you think about this this is a form of semistatic adaptation you were mentioning before that you know fine is happening a specific moment in time so it's a snapshot of what the situation up to that particular Point sometime this works really very well there are situations where you want have more Dynamic information and so perhaps have some information is provided in context uh when you do inference so when you submit the prompt and there are techniques like retrieval argumentation of based on search can help a lot or also you know in context learning where you provide this additional information uh at prom time perhaps using techniques like caching to improve the efficiency reduce the cost I would say that there is a there are multiple options to optimize and really depends on the specific spe ific use case but I see a balance between this happening more this different dimensions happening more and more awesome thank you thank you for the great answer uh I I I agree that the um I'm just going to reshare my slide deck really quick um I agree that the uh the fine-tuning for um fine tuning for large language models can get significant gains that maybe you not uh maybe you might not be able to get uh just by using the base model um so so uh lots of options for for fine tuning both offered through Google Cloud as well as open source Frameworks like UNS sloth um uh so so hopefully um hopefully that uh that will get addressed um next question kind of related uh uh for for Christopher to answer um is will a single Superior model solve all health problems um this seems like an ambitious goal even for even for large language models and for generative AI yeah all health problems is a very broad scope um what I the way I like to think about it is to take a little bit of a perspective before ji so if you just zoom out over the last couple of hundred years um clinical studies did not exist a couple of hundred years ago vaccines did not exist a couple of hundred years ago um uh uh uh regular visits for for um for preemptive care did not exist a couple of hundred years ago so many things that we are using nowadays in modern healthc care did not exist in the past and had to be invented and when I hear about people thinking about using gen and Healthcare I hear a good chunk of use cases where they basically try to make something more efficient or make something better like use this and this to help the doctor write a reply to the patient faster um use this and that for the patient to help them educate themselves better um use this for medical imaging tasks and so on and those are all good and I think it will improve things but what I'm waiting for is both the scientific discoveries but also the different Pathways of care that are possible now that would not have been possible before model like gemini or met gemini or met pal so this is really the the big step in healthcare that I anticipate to happen in the next 10 years um where I'm really hoping for a lot of Innovations and and inventions to come along um will healthc care be solved at that time uh no that's a journey for Lifetime uh but it will get closer um so but my short message is incremental improvements and efficiency improvements are important but they are not the only thing we can do and thinking different as in what is possible now that was impossible to think about in the past those are the things we need to explore more I love it and that's very inspiring it also seems to it seems to hint that we're just at the beginning of this journey and that we still have a lot of untapped opportunity for people to apply AI in the healthcare domain um as well as to many other specialized domains um so so really really excited to see what the next few years hold next question um uh another one for security um it's inherently an adversarial field um does the adversarial nature change your strategies for where and how you deploy large language models um for security purposes and how has this changed over time um you know you you mentioned I believe Scott that you had started doing this a couple of decades ago um how has the the sort of perspective on where large language models do their inference changed over time yeah that's a that's a good question um yeah I'll I'll give UMES here a chance to to respond as well but you know I think the the big thing is that you know a couple decades ago no one even saw the adversarial you know sort of vulnerability that ml could provide right I think I guess now it's like 15 12 years ago something like that some of the first evasion papers came out and people realized that oh wow this is actually possible to just get around all of this ml that we're using and I think it you know for to some degree the llms are an extension of that right it's a it's a new um a new attack surface so to speak uh that we now have to continue monitoring and and kind of look out for how we incorporate into our tool chain yeah I'll add to what Scott said if you build any kind of application you have to think about security and whenever you take dependencies on external sources of things then you need to consider you know what where's that coming from what's the likelihood that it could be compromised or malicious and that includes things like if you include open source libraries in your code right um it includes if you're taking input from a user that might be trying to do something bad to your system you have to sanitize the input and use it carefully and correctly all those same principles still apply in this world it's just that there's new and different ways that people are going to exploit them so for example you know maybe it's not the open source well it might also be the open source code that you're using uh but the data that goes into training a model very often is sourced from external places right because you can't generate enough just in yourself to get the results that you want and so you have to be very careful about you know cleaning that data ensuring its accuracy ensuring it meets your safety standards and so forth uh the runtime part also exists right so in the same way that people could try to craft kind of malicious inputs to systems you think about things like SQL injection as a classic attack of that kind where you're injecting something which you know Downstream will end up in a SQL query and someone wasn't careful and now you able to kind of change the query to be what you want to do away from what the application author wanted to do um the analog of that roughly is prompt injection in the world to BMS and it's just that instead of being SQL it's natural language and what's particularly challenging about that is both the user input language is natural language and the programming language of the LM is also natural language and so when they come together it can be hard to isolate one from the other and ensure that it's doing what you want considering this user input but not in the wrong way and you know what there's no fundamental fix for this yet people are trying lots of techniques they're trying to make it more resistant to these things uh by training away some of it um and by just doing sort of puristic scanning of inputs does this look suspicious right um but other techniques that we're employing inside the system such as decomposing a problem into smaller parts where many of the maybe that user input only goes to like one of those subcomponent components and if you can say confine that particular input to that subcomponent and the output of the component is is something more like structured data like what is the likelihood of this being malicious 0 to one let's just say as an example then you've effectively conf find the impact of that input to just it can affect this number and of course you'd like the number to still be correct but at the worst it'll be the wrong number but when you look at the broader analysis that the system is performing it's not going to affect like the overall plan and the instructions that are actually getting executed right which is the kind of thing that in in general a prompt injection can do and so you know we're actually looking at a variety of these kinds of techniques and it's in an area of very active research I love the I love the sort of example that you gave of using models almost to help with the um to help with the the uh defensive testing of hey you know here's an input from a user does this input look malicious or not um and then uh and then also decomposing the the higher level tasks into steps um to reduce the likelihood that if there is something malicious that it would impact the entire system as opposed to just a small part um that's really that's really interesting and I think it points towards how people need to completely rethink how they architect systems um in this in this new world of large language models um good very very cool so next question this is uh this is for Christopher um what are the ethical considerations when creating specialized large language models for sensitive Fields like healthcare um and how can you ensure that patient privacy and data security are maintained during training um and also deployment um so so uh you know especially if you're working with things um uh uh that might be usually interfaced with as a rest API as opposed to um kind of collocate locally on on site at a hospital um how are youall thinking about that yeah so for the answers are um I will answer training and deployment separately quickly so for deployment um there's we many countries local regulation you can rely on where you're serving infrastructure simply has to be compliant with that so us for example has the Hipp framework and and Google Cloud infrastructure is is um compliant with that so that ensures that the data privacy for the patient data or other data as well is handled appropriately during serving time um as you might use the data as part of your pumpt and so on for training um there's we usually use the identified data or synthetic data so again there are standards per canway on what this precisely means but you can see the basics are obviously you know no names dat shifting and stuff like that and that is for two reasons one is because it makes a lot of sense from a patient privacy perspective uh but the second thing is also there something about data hygiene like the more detailed individual data you have in your training mixture the more likely the model might pick up not only the general patterns but also random OD balls that lower your performance of the model so um deriving the wide level of abstraction um not only with deidentification but also with proper aggregation and so on is crucial for model performance and also crucial for making sure that no personal data leaks into the model itself awesome that's uh that's great to hear and I love the call out towards some of the additional security best practices that uh that Google Cloud employs for for making things typ compliant and making sure that you know data is encrypted in transit and all of that good stuff um excellent um next question uh also for our secm team um what do you think is the most effective use of large language models in solving security problems and why what has been some of your favorite use cases for AI in the security domain uh why don't I start and then Scott can pick it up as well I think Scott really touched on a a key Point earlier which is um we see llms really unlocking the ability to perform security tasks like that procedural knowledge that we can teach it uh being even more powerful than kind of the you know factual knowledge that the thing might have because in any case you can't rely on the factual knowledge that you think in llm as anyway right you need to ground it in something authoritative if you want it that thing to be taken as true you have to say it right there in the prompt U to have a high confidence right and you want to make sure that it's not relying on potentially you know hallucinated information as well and so we take a very strong grounding approach to anything factual and what we really are seeing is that there's whole workflows where you know we're like okay what is a person doing all the time that they would rather not be doing right that we think one of these systems could do and maybe actually we could do 70% of it right we have kind of good data sources we have some historical examples we have some code you regular deterministic code that can do a lot of it but there's some key part that it can't do right like you know render some aggregate judgment among multiple things figuring out how to use the tools that we already have available effectively being able to do that iteratively until you reach a point where you're ready to make a judgment those kinds of capabilities are pretty hard to write in regular code and so I I think we're really seeing a big Improvement there um another one is definitely in the area of code you know code analysis I think Scott alluded to for analyzing potentially malicious things it's already proving to be very effective and then for code generation that's another one of those big missing blocks right so I like to say every year someone has the idea of oh we already know that people have all these you know potential misconfigurations in their cloud deployments like they've over permissioned some account we should just fix it for them okay but the problem is that actual fix means you have to generate a pull request to modify a terraform file that's in the customer environment somewhere right and that is just not a thing we were easily able to do because everyone writes their terraform a little bit differently and it needs to be a contextual change that has a certain uh impact but with the Advent of LMS that is something that we now believe we're going to be able to do and so that's very transformative even though it's like in something that's one block out of the whole thing it's what makes that end to-end thing possible yeah I was just I I'll just add in like my my view of it is very much that the llm is sort of your substrate right it is the thing that's the connective tissue among all of these diverse systems that exist inside of security um because that level of diversity is exactly what causes all the pain for for most actual humans right the fact that you have to swivel between that system and that system or that data Silo and you need to find a way to get it to merge with that data Silo and it's exactly what llms are great at right leveraging the apis and the tools that are available to them and being able to combine all this heterogenous data in a way that's coherent and reasonable given enough you know prior experience with exactly what UMES was saying like this procedural knowledge about what exactly you should be looking for and what it means for your final determination and and I I think it's also important to point out that uh when when we refer to customers for Google Cloud we mean whole organizations whole companies um and so uh just knowing that the models are able to flexibly uh you know change and modify their their outputs and their recommendations at the at the company level um uh is is really really huge as opposed to having to have dedicated charistics and rules made for each each particular company um so that's huge I uh this is also a nice tie in to our next question which is uh what is still one major Gap that you see in large language model capabilities that make it uh uh uh that uh is vital for success in the security domain but is a place where the models just aren't there yet and we've touched on this a little bit throughout the duration of the of the chat but would love to would love to get your perspective explicitly yeah no this is a really good question because you know for anybody who's worked in the security domain um you'll probably come to realize that most people who work in cyber security are by their very nature incredibly skeptical about everything right like unless there's clear proof that something has happened uh you know they're always like oo I should go look into this further right it's actually been one of the most difficult points of getting ml broadly speaking into this space is this idea of explainability and can you trust it and how do I verify and I think llms just add a new dimension to this right so we mentioned this idea of citations or like grounding that's certainly one way of thinking about it right that certainly goes partway in my opinion the problem is that with grounding in a lot of cases especially in the way it's usually used in citations is um that that human almost has to do almost as much as they probably had to do to start with to verify all the things especially for highly technical Fields right so like if I'm citing all these you know threat actor reports or different reports that are coming out from mandiant or other organizations then like I either have to trust that in fact this statement that is being cited is is there and it is the implication of what's in the report or I need to go back and just basically read the report myself in which case you know how much time am I really saving so I think there's definitely like a broad Gap here which is like how do you shortcut some of that stuff how do you make it so that even in a space where you know it's hard to validate or verify in some ways we can minimize that amount of time to for verification or even do things like provide a really deep you know understanding like a confidence score I know a lot of people think of like log likelihoods right but that's not really quite it because that's just your probability of producing a particular output token sequence what I'm saying is like can you actually bake in you know your confidence in the underlying grounding data your confidence in the reasoning that you applied in the same way that a human analyst would do these sorts of things and I think that's the Gap where if we can make that change that that's going to unlock uh a tremendous amount of value for customers and users of these systems yeah I mean Scott gave a a fantastic Big Picture answer I'll give a maybe a slight smaller Picture answer as well um in terms of capabilities so um again just a refer to what Scott said in his previous answer the ability to stitch together tools data sources apis to pull everything that's relevant and then render judgment is absolutely critical to solving most security problems and today uh tool use by llms is still fairly rudimentary and takes a lot of kind of hand tuning to get it to use those tools effectively right it's not a case where you can say here's the API here's some examples you know go use these things effectively and that is particularly acute because in many cases the API services that we're dealing with are very wide meaning the schemas that are used to describe information can have hundreds or thousands of fields and each one of them can have some semantic meaning and a lot of the stuff that we've seen in you know tool used by LMS today is like very I would say very simple R rudimentary apis where it's like three parameters they strings you get back a sort of simple answer right and even that has proven difficult to integrate across apis in a way that doesn't require a lot of handcrafting for each one I feel like if we could crack that nut and in a scalable way just start handing tools pre-existing tools to an llm and it could actually understand and use them that would be that would be gamechanging yeah so that is something that that folks on this course have been learning about a little bit earlier in the week about function calling and tools use and learning just how uh just how challenging it can be to get models to use tools reliably and to know which tool to use and when um so I I think this is another inspiration that in addition to better benchmarks and evals for the medical domain um hopefully folks are starting to think about better evals uh for for tools use and for function calling as well especially for composite tools use and function calling um excellent well thank you thank you both so much and our final question um uh for Christopher all about medp Palm I I I saw the medp Palm evaluation white paper on diagnosing depression and PTSD using client interviews as the basis for its assessments how has this work of evolved since then and it has been I I remember when the med Palm paper first came out it's been a while um and while in the large language model space translates to almost like an eternity um how have things evolved since medp Palm Chris well yeah that paper uh I think was like middle of last year or so if I remember correctly um so for context the scenario was you take like a transcript of a patient doctor conversation and you given the transcript what is the likely diagnosis and it performed decent I'm sure by now it performs much better um but uh the way again similar to the answer with the med QA question it's all about expanding the scope and moving to a harder task here so for instance from a transcript to a diagnosis it's not an easy solve task yet but it's getting much better than it was like a year or two ago um but ultimately what matters for patients is not really I mean it's also to get the correct diagnosis but what really matters is that the patient is going to do the best next thing for the health right so that is not only knowing the right diagnosis and giving the right recommendation but also helping the patient understand so that they're more likely to do what is best for them incorporating the life situation and everything else um so we started research along the lines as well here that's our research system Army am um which you can find more about um where we try to evolve um llms not only to take a transcript and come to conclusion but also to be the chat partner in the transcript and again all of this is really early Explorations like even if the model performs well there's still the question of demonstrating it in real life environments and clinical studies in a safe way that you can see how it really makes a real patients um effect in a positive way and so on so we are still in an early part of that Journey but yeah it's exciting um and I can't wait to see what's going to happen next I love how patient focused you are about uh you know we're using these really powerful models but at the end of the day the thing that matters most is whether or not it made a an impact that was positive in people's lives um so on that uh on that really hopeful