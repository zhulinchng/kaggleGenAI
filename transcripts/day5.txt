so question number one for Gabriella how has mlops changed with the introduction of large language models in generative AI you know I started doing machine learning in 2009 as an engineer and back uh back when we were just building single task models and doing uh kind of more traditional machine learning um it felt like there were a lot of nuts and bolts that had to be considered um now it feels like the the game has changed quite a bit so so tell us how how you've experienced mlops to change over the last couple of years yes I also started doing machine learning around that time around 2010 and I do remember in those early days the model development was rather manual with deployments been ad hog we probably all remember you you started uh back in the days like me uh sharing data and models with a USB obviously this approach like scalability reliability and the collaboration was rather bumpy right if we just think how the field of mlops has matur we started to incorporate and extend the principle from devops to automate the building testing and deployment of both data and model assets right um this was very necessary because the dynamic nature of machine learning where the data changes and model requires continue monitoring and retraining well I mean over the time probably like late 2010s um a white tool of ecosystem uh for mlops arose and then we were able to manage the entire ml life cycle also the rise of the cloud was very important to further accelerate the adoption and of automated pipelines and also serverless machine learning this actually helped us a lot to to increase the scalability the liability while decreasing also costs for production and and experimentation now more recently with the rise of generative AI it has pushed again mlops to adapt uh I think now we are incorporating new roles like prom Engineers AI Engineers we have a broader artifact management including model configur figurations Foundation models Ed prompt templates chaining pipelines Etc and also achieve towards a holistic application monitoring before we were focusing only on the model on the machine learning monitoring monitoring the machine learning model sorry but now we are focusing on monitoring thenar andn application uh we are also thinking about encompassing user feedback and doing task specific and custom evaluations of the entire system and of the singular single uh individual part Parts moreover you all know uh we have new models and new Frameworks arising every day so it also demands exceptionally agile workflows I I love that you called out the the broad spectrum of artifact management that teams have to deal with today both in terms of you know doing things like defining functions but then also monitoring prompts um how prompts change over time the the chaining templates um it's really really fascinating to see how some things have gotten a lot easier um but a lot of other things have gotten much more challenging to manage so I I love uh I love that detail and and also really really excited to see somebody else who's like one of the ogs of machine learning um that is that is still like interested in this new Brave generative AI World um awesome um next question um for an not and for Olivia uh tell me about the importance of evaluation for productionizing models is it only possible for text or is multimodal evaluation also an option um and Olivia I know you're leading one of our evals teams over at Google deep mine so welcome really excited to have you here thank you Paige and yes uh I think it briefly start off a mentioning evaluation since this topic was brought up quite a lot in the last days and questions in Discord as well and I know I got a lot of you okay come to day five so uh hopefully you had a chance to read through the evaluation part of the white paper uh but evaluation has a lot of Dimensions to it um I'll start off with mentioning some parts of taex evaluation and only be a to dive deeper into multimodal and um uh the structured like disassembly of various tasks for evaluation so um evaluation uh depends long story short for text evaluation uh it's um it's incred an incredible uh part of uh mlops you cannot have just like how it was a predictive AI as Gabriella mentioned uh you still need to evaluate however evaluating a generated image or string value right a response is very task specific and it becomes far more complex it's not just a singular value that you can kind of calculate numerical value can calculate accuracy on anymore it's far more toss specific and nuanced so uh on the text evaluation side of things um we have um we can use this the traditional techniques which kind of compare like um things at a document or sentence level where uh using computational metrics like say Blau or rou scores where you kind of compare the generated sentence but given that you have an existing ground truth or an existing golden human like uh thing to compare to so it only exists as okay how how good is the generated answer compared to the golden answer which I was expecting it to be so um the I so you some uh so it can be used for machine translation and many other tasks the problem with this approach is that um it's comparing to one go ground truth answer so you need a ground truth answer and secondly several toss like summarization and very TOS spef specific answers are very hard to compare um as a unigram or engram basically token basis subsequence of the strings and compare with an one existing answer because there's many ways to write a summary which might all be correct or even better but it's really hard to judge that by solely comparing it with an existing reference point now there is where llm as a judge or autor rator based systems uh come into play and um for text this is also very important because you can uh either take an llm where uh to judge another lm's response and say okay let's say you had a prompt you got a response from an llm you can have another llm which is kind of um specialized at evaluation judge it on a say on a scale of 1 to five and and uh how good is this response compared to what I like given the prompt and the instructions and and explain why so this is kind of like you can uh judge it as or sometimes um you can have uh they can be a tie and and two responses can be very similar alterated from different models or settings and you would like to compare side by side comparison which is how humans evaluate things it's like take two responses to a prompt and say which one of these two are better because humans when given a bunch of responses do not tend to look at all the responses at once so and they tend to kind of compare in pairs and this is where side by-side evaluation is also possible while you compare pairs of um uh responses at once and yeah there's a lot of um ways you can do that you have uh Services uh like vertex AI which offer an evaluation service but also as you mentioned earlier page prompt Fu and and other uh um open source alternative to it as well so Olivia uh has worked on a lot of great multimodal evaluation and other techniques I'll hand it over to you Olivia for diving more deep into that thanks Anan um yeah so hi I'm a researcher at Google deepmind and I've been working in the evaluation space and in particular in the multimodal space um and just to kind of add to what Anan said I think evaluation is a really key area in NL but it's actually become a really interesting it's always been an interesting research Direction but it's become even more interesting with these large models that are just being used in such a wide variety of settings and for different capabilities and has become really difficult to even think about the evaluation problem how to break it down um it's something that I've been thinking about for quite a while um and in particular kind of the first thing that I tend to think about is what use cases do you really imagine your model being used for and can we then build like an evaluation data set that has uh coverage over these kinds of use cases and I think it's really key to think about building this data set as it's going to be what you're going to be evaluating your metric and your models on so if it doesn't have good coverage and if it's not really covering the things that you care about then you're not really going to know how your model is doing and you're not going to be able to monitor how it changes over time um and so maybe it's not the most exciting part of the process but it's a really key and fundamental part of the process um and so kind of what once you know what you want to evaluate and you have sufficient coverage and sufficient confidence um then it's time to actually build your evaluator and an did a really great job of talking about the different ways protects I think multimodal is particularly interesting as you have you know image input and text in input and you're not evaluating them in isolation but you want to evaluate um the their combination and I've been looking mostly at image generation and video video generation which is particularly interesting as there's there was never a notion of kind of a ground truth you never there's never a ground truth image that can be generated from a given caption and so historically it wasn't very easy to evaluate these models and people relied on simple things like clip or FID even though as models became better even maybe two or three years ago people knew that these weren't fit for purpose and one interesting thing that we've been looking at in our gecko work is how we can actually leverage Gemini models as black boxes and see whether they're like effective for this kind of pass and we particularly looked at alignment so we were looking at you know if I have a prompt and I generate an image then is that image aligned with prompt so for example I want to generate an image of a dog wearing a top hat is the image generated does it actually the dog is the dog actually wearing a top hat Etc are the properties in the prompt actually in the generated images and we could follow an aerator type approach which is um potentially very useful for many scenarios but we wanted to get away from just generating a single score and think about how we could give the user more control and um understanding what the metric is doing and so what we did was we thought about how can we use a Gemini model to break down this task into sub questions so for the prompt I was saying you know is there dog is there a top hat and then given these questions we can ask the gem model to answer the question give the image and then if there's know no dog and the answer would be no and then what we can do is that we when we give this final metric we can also give this uh breakdown to the user and then the user can actually see you know where's the where is our model doing badly and then we can give the user a kind of intuition and an ability to have some flexibility with the metric they add questions remove questions or evaluate for additional properties and I think this is like really quite an exciting approach that we found to be quite effective um to compare models and also multiple generations for a single brand it's also quite General so we're thinking about how we can generalize it to other modalities or combinations thereof and we're excited to see where it goes and how it can be useful and I just want to finish by saying like that while this is One Direction of work I do think that there's a lot of interesting uh work that's been happening in the space more generally both in Google and externally and I'm excited to see how we can use in multimodal kind of the work that's been doing in text in order to push it absolutely and we can definitely add a link to your research um particularly gecko um uh as well as some of the other really uh exciting multimodal evals that deepmind has been creating uh in the video description um I love your uh your calling out that you can use Gemini almost as like an aerator to help uh to help with some of the multimodal evals um and I saw another uh a blog post just recently posted to around video understanding I think it was the Neptune um the Neptune Evo that would be would be interesting to call out to so uh so I love uh I love this Brave New World of being able to to do multimodal evals um including image generation and video generation evals and using models to to kind of help with those tasks it's amazing um so thank you thank you for that great overview and uh and also for pointing out that um evals are only as good as how well you specify your problem um and how well you you kind of architect your eval to ensure that you have coverage over the thing that you care about and the thing that you want to be measuring um it's super important and I don't think enough people spend uh spend time thinking about it um so it's great to to reinforce this in the in the educational content cool um next question um for Socrates um what ml Ops challenges are no longer priorities given that many companies are now using rest API calls as opposed to training deploying and maintaining their own models we talked a little bit about this already um with Gabriella I think but what has been your perspective on um on what things are are a little bit easier now um uh given that most companies are are calling models as rest apis sure calling a model like rest API means like somebody else has done the heavy lifting for for us right that means that someone else has prepared a massive amount of data to train a model evaluate the model by checking a loss function and how close is to the results that we need to have uh someone else has passed through all the different steps of auditing what data and artifacts we use to produce a model so all this will disappear by calling a model like a a rest API of course uh the customers or everyone who is using the the foundation models can jump directly on building an application an application that means that they don't really need to have data science skills extensively except if they start fine-tuning a model uh of course having prompt Engineers AI Engineers that are the new skills we need a little bit to see how we can move the data science and ml Engineers to that space but uh I think if they are experts in their domain and the application they want to build this is relatively easy now as you can imagine the classic approach that we had was Data preparation training evaluation Etc a very long process now we start directly with evaluation and this is a big benefit why because we can reduce massively the amount of time to build our application of course in the evaluation we have uh new parameters as anad said earlier right we cannot rely on uh how the loss function performs but we need to rely on toxicity factual knowledge and other more task specific metrics the next one is about monitoring now we don't need to have all these data or model drift classic ways of thinking about we need to have more about how our model drifts in terms of toxicity or function knowledge or other task specific metrics that we have and lastly is about guard rails right uh what I want to say is in classic machine learning we used to to have specific guardrails about the specific number limits if I may say or thresholds now we need to be closer to the understanding of the use case and to set more limits or more rails around the specific topic that we are examin awesome I I love uh calling out all of the new things that we need to learn and and to kind of be uh considerate and thoughtful about and how user focused all of those things um all of those things are um so thank you for thank you for um pointing out things like tox toxicity uh and then the other the other components of model deployments that um are are really really kind of mission critical important to get right um uh and that that we're building tools to help users um figure out how to Wrangle about the model deployment that you mentioned and I forgot to to to point out right in classic machine learning also someone needs to be responsible to deploy the model and ensure autoscaling mechanisms that end point is up and running continuously now we don't have this concern right somebody else has deployed that and makes sure that the model is available for everyone thank you for this yep the auto scaling Auto scaling is something that Google cares very very deeply about and so just for my personal projects I really really love that I don't have to to worry about deploying an open source model on some sort of server myself um and that I can just rely on all right well if I I get a a lot more kind of people using my system um then it will gracefully scale along with the number of user requests as opposed to me having to figure out how to do that balancing um cool awesome so next next question um this is the perfect segue into vertex AI um and to uh to to kind of cloud ai's offerings um advate welcome um tell us a little bit about vertex Ai and about why a customer might want to use it at their company yeah great uh hey everyone a Parker I'm one of the product managers for vertex Ai and vertex is an Enterprise AI platform that's meant to be kind of like your One-Stop shop for everything when it comes to machine learning so that means both generative as well as non-generative type tasks that an MLT meta company might want to do so if you need access to the API for Gemini but also other models like models from anthropic or from llama met as llama models or mistro models you can get access to all those models and then start to think about how are you going to ground those models use them for generative applications or building out agents um and around this you need a whole set of Tools around mlops a lot of the the topic of of today uh but the same same Concepts still also apply in non-generative use cases so if you want to still build a Time series model or anything that is non-generative in nature you can still use vertex AI for all your needs um now depending on where you are in your skill set and your journey inside of machine learning you can either build everything by hand by having managed notebook environments managed Compu to train your models experiment tracking mlops pipelines that you don't have to run that actual underlying kubernetes clusters uh uh for or you can take uh a simpler approach by using something like automl for automl it's a pointand click interface you don't necessarily need to know that much about machine learning but understand how to evaluate the quality of those models and provide the right quality high quality data so it starts to abstract away a lot of the effort that you would have done by managing a lot of the infrastructure you don't want to necessarily be a devops engineer perhaps you prefer to focus on ML um and and you can start to tackle various challenges whether they be generative or not yeah that's a great overview you know we've been talking a lot about um kind of Open Source libraries and and kind of things that you can stand up yourself and and kind of run on on commodity Hardware or infrastructure um but there can be immense value in time saving and and kind of U bundling it all together and then having managed services for all of those systems as opposed to trying to maintain and upgrade and modify all of those products yourself um so really love that overview of vertex is trying to help people focus on uh the value ad focus on machine learning or or uh kind of the impact to their business as opposed to to to trying to figure out how to Wrangle all of the services um and deploy them themselves um awesome next question for ver um what specific mlops practices should be prioritized when starting with generative AI um and how do these differ from traditional mlx workflows um and are there any beginner friendly tools that help to uh help to kind of manage these practices for foundation models on vertex great question so I'll actually build upon what so where Socrates and AD are left so in the world of predictive AI the models are typically self-contained and trained for specific tasks right in the world of generative AI on other other hand models and particularly llms um they are more versatile they require a prompt to guide their behavior and output so this difference uh it necessitates considering the pro Ed model component as the fundamental unit of gen mlops so and this ENC encompasses both the model and the front so coming to the question so which mlops practices to uh to prioritize when starting with Gen on vertex AI specifically right first is think about model Discovery efficiently identifying the optimal Foundation models for your use case from a vast landscape of options is cre crucial so this entails a systematic approach to evaluating various models based on factors like quality or latency uh development time cost and compliance so we have to evaluate S factors so that's the first step uh second think about prompt engineering this is iterative process involves crafting and refining prompts to elicit the desired outputs for the model it's crucial to manage prompts as both data as well as code um utilizing the data Centric practices like validation drift detection code Centric practices like Version Control and testing so it's not just first time it's like how you how you go back and F tuning the propit third think about chaining and augmentation right as llms can struggle with recency and hallucinations chaining multiple models together and integrating external apis and data sources is very essential so uh now uh I I'll also touch upon the tooling that Vortex a offers in this space like we have we offer tools like grounding extensions we have Vector search agent Builder to support these kind of needs next is uh model tuning and training in this context Vex AI provides robust platform for fine-tuning llms supporting techniques like supervised fine tuning reinforcement learning with uh human feedback and distillation managing the artifacts measuring the impact of the tuning and leveraging tools like uh vtex AI model registry and other tools that actually support within the uh Google cloud like datax are essential for this process um as we think about data right think about data practices as well so generative AI applications leverage various data types from different data sources so implementing traditional mlops and uh de devops practices is like vital to ensure this re reproducibility adaptability uh governance and continuous Improvement across all data types involved next is uh evaluation and the uh others have covered a lot on this topic I'll just quickly talk about it evaluating generative a models requires both manual and automatic automated approaches right uh custom evaluation methods and metrics are often necessary to this is due to the complexity and subjectivity of these model outputs so addressing adverse serial attacks is also crucial vertex AI provides tools like that we already discussed about but these are automated metrics again coming to automation automatic side to side comparison to evaluate different models and the rapid evaluation API to support this kind of a process next is uh in terms of MLS now we are thinking about deployment so deploying generative AI systems involves managing various components often like using standard software engineering practices like Version Control cicd so in in this space Vortex AI offers end points for deploying models and features like citation Checkers safety scores watermarking uh content moderation tools but in addition if you think about the bigger Google Cloud we also have tools like Cloud build Cloud deploy for cicd some of these were actually part of the of EDS demo um earlier um next once the once it is deployed into uh production now we are thinking about governance this is where establishing control accountability transparency over entire development and uh deployment life cycle it's all critical so this involves governing the chain element life cycle um uh data tune models and code all everything that we have covered so far needs to be governed tools like vertex AI feature store model registry and datax play significant roles in this aspect so I think I covered overall MLS process that applies to gen uh on how how that changes as well as I touched upon a bunch of tools but to summarize the tools for the beginner friendly tools is what being it is being asked in this question I would say start with Vortex AI model Garden which is where you'll pick up the model of your choice next uh for beginners Vortex studio is extremely helpful because it features a playground where you can exper uh experiment with different forms and models and making it ideal for uh the beginner uh environment uh eventually you'll also get into agent Builders I mean this is a place where you can build your own agents generate UI agents right conversational chat Bots and things like that and then uh for the mlops itself we have Vortex AI pipines so yeah let's stop there awesome thank you so much um and I I definitely uh I definitely think that there there are many different products that are available on vertex AI um for folks to take a look at um what are some strategies uh Gabriella um to monitor large generative AI models um in Google cloud and production environments with high variability and user queries um would love to learn more about this yeah when we think about monitoring generative AI models especially those ones with high variability iner queries we need to track two things uh from one side we need to track the system performance and by System performance I mean the latency how fast are the responses um the query size how big are the queries and these other aspects the other part is to also ATT track the the model quality itself like are the answers accurate relevant safe Etc like the most reliable way to achieve this is deta like having detail LS of everything that happens including like user queries retrieve Snippets uh prompts generate answers user feedback lences Etc so one thing we can do is store all of this data in Google Cloud um bigquery which is a fully managed and seress data warehouse why we query is because it is like a cost effective way to keep uh this data analyze it and create dashboards to visualize the performance and identify um areas of improvement um Etc like just to give a concrete example for the case that you are presenting here is like once your data is in B query you can create embeddings for the user queries and model answers and then group similar ones together you can analyze then the user feedback for a specific cluster of queries um and then bigquery also works very well or integrates with other tools like looker which will enable to to build a dash for for monitoring and it will give us like a clear aggregated view of both the system and model performance including metrics and feedback for a specific um query of clusters another thing we can aggregate here is like you can also leverage open source tools for observability and then couple them with Google Cloud trace and Google Cloud loging in a way that you can instrument every answer given by the uh generative AI model and you will also have a detailed tracing and actually this information that I'm saying was also presented um or is part of the um demo that U IIA presented earlier yeah I love uh I love big query as an option to store logs and prompts and data about model interactions uh especially given how many wonderful kind of Integrations big query has introduced with things like looker um and also sheets and then uh even the machine learning features that have been added into big query um I I really love being able to use Gemini to help me write SQL queries to interact with with big query a little bit more effectively so I love that answer um and if folks haven't experimented with big query there are a lot of uh there are a lot of great resources to get started we can add some in the YouTube video comments as well um excellent uh next and final question um uh for Socrates and for advate how does vertex AI enhance mlops for foundational models and generative AI applications are there specific features that make it more suitable for generative AI um compared to other tools or platforms and we've covered um some of these already but would love to get your perspective yeah absolutely if you think about it a lot of the same Concepts that applied with mlops in the past still translate when it comes to generative applications um so if you start with let's say something like prompt engineering conceptually it's actually kind of similar to uh let's say if you're doing hyperparameter tuning it's something where you in the past would have manually done it You' have to tweak it to see how you get the best performance uh today you don't have to necessarily do that um inside a Vertex a we have our prompt optimization tool so you bring in a data set an eval data set and it will do the optimizations of your prompts on your behalf simplifying and removing a lot of the the art of prompt optimization and making it a little bit more statistically driven in nature beyond that everyone's spoken a lot about evals uh I also share the belief that EV vals are kind of like the key thing that you need to land to get high quality results um so inside of vertex AI you can do evaluations both to compare models to compare prompts uh to see how your various different parameters that you select uh impact your results and also see how fine-tuning impacts your your model quality um and then finally there's a lot of thought around well once you're in production how do you actually monitor the performance of this model this is a bit more of a an emerging space specifically for generative AI um but we're rolling out an experimental capability to to evaluate and monitor models in production to see what topics are starting to happen uh are they adhering to safety uh protocols that you've defined from your safety filters and how can you make changes to the performance of the models by going back to those prior steps and augmenting your prompts or doing things like fine tuning which you can do all inside a vertex and to add to advate right uh of course we have some other toys in gcp that can help everyone to uh industrialize their in AI applications for example you can combine the vertex AI Val components that theate mentioned with pipelines and experiments and to create a a quite scalable experimentation environment for geni or we have prom manag prom management that helps us a lot to store our proms understand what went wrong uh reuse them or try uh and change change them after evaluation and then we have a prompt gillary that helps us a lot in order to start with a a nice version of a prompt for a particular application awesome I I love the uh kind of the the end to-end solutions that are that are offered on on Google cloud and then also um how many of them are built to scale effectively um as the number of users grow for Enterprises and for companies I I think that's that's kind of mission critical important and something that is underappreciated until companies do need to scale um so it's it's great to hear great to hear those options um and then next uh thank you all so much for uh for being wonderful kind of expert hosts and uh guests uh

above is the transcript for the Q&A session that consists of the following 7 questions:

- How has MLOps changed with the introduction of large language models and generative AI?
- Tell me about the importance of evaluation for productionizing models. Is it only possible for text or is multimodal evaluation also an option?
- What MLOps challenges are no longer priorities, given many companies are now using REST API calls as opposed to training, deploying, and maintaining their own models?
- Tell me a little bit about Vertex AI, and why a customer might want to use it at their company.
- What specific MLOps practices should be prioritized when starting with generative AI on Vertex AI, and how do these MLOps workflows for predictive practices differ from traditional models? Additionally, are there any beginner-friendly tools or workflows within Vertex AI to help adapt these practices for managing foundation models?
- What are some strategies to monitor large generative AI models using Google Cloud in production environments with high variability in user queries?
- How does Vertex AI enhance MLOps for foundation models and generative AI applications? Are there specific features that make it more suited for generative AI compared to other tools?



